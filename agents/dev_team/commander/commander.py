"""
AI Commander - æ™ºèƒ½ä»»åŠ¡æŒ‡æŒ¥å®˜

æ•´åˆèƒ½åŠ›æ¢æµ‹ã€å…±è¯†æœºåˆ¶ã€äº¤å‰æ ¸æŸ¥ç­‰åŠŸèƒ½çš„é«˜çº§åä½œç¼–æ’å™¨ã€‚
"""

from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
import json
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed

from agents.dev_team.commander.base_orchestrator import BaseOrchestrator, default_agent_factory
from agents.dev_team.commander.capability_detector import CapabilityDetector, CapabilityProfile
from agents.dev_team.commander.consensus import ConsensusEngine, ConsensusResult
from agents.dev_team.commander.cross_validator import CrossValidator, ValidationReport
from agents.dev_team.interfaces import Agent

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)


class Commander(BaseOrchestrator):
    """
    AI Commander - å¢å¼ºç‰ˆåä½œç¼–æ’å™¨
    
    åœ¨åŸºç¡€åä½œç¼–æ’ä¹‹ä¸Šæ–°å¢ï¼š
    1. æ¨¡å‹èƒ½åŠ›æ¢æµ‹ä¸æ™ºèƒ½åŒ¹é…
    2. å¤šAgentå…±è¯†æœºåˆ¶
    3. äº¤å‰æ ¸æŸ¥ä¸è´¨é‡ä¿è¯
    4. åŠ¨æ€ä»»åŠ¡åˆ†é…
    """
    
    def __init__(
        self,
        config: Dict[str, Any],
        output_dir: str = None,
        code_executor = None,
        agent_factory = None,
        enable_capability_detection: Optional[bool] = None,
        enable_consensus: Optional[bool] = None,
        enable_cross_validation: Optional[bool] = None,
    ):
        """
        åˆå§‹åŒ–Commander
        
        Args:
            config: é…ç½®å­—å…¸
            output_dir: è¾“å‡ºç›®å½•
            code_executor: ä»£ç æ‰§è¡Œå™¨
            agent_factory: Agentå·¥å‚å‡½æ•°
            enable_capability_detection: å¯ç”¨èƒ½åŠ›æ¢æµ‹
            enable_consensus: å¯ç”¨å…±è¯†æœºåˆ¶
            enable_cross_validation: å¯ç”¨äº¤å‰æ ¸æŸ¥
        """
        if agent_factory is None:
            agent_factory = default_agent_factory
        super().__init__(config, output_dir, code_executor, agent_factory)
        
        # åˆå§‹åŒ–Commanderæ¨¡å—ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼ŒèŠ‚çœèµ„æºï¼‰
        self._capability_detector = None
        self._consensus_engine = None
        self._cross_validator = None
        self._commander_config = config.get("commander", {})
        self.mode = "auto"
        self.enable_capability_detection = True if enable_capability_detection is None else enable_capability_detection
        self.enable_consensus = False if enable_consensus is None else enable_consensus
        self.enable_cross_validation = False if enable_cross_validation is None else enable_cross_validation
        
        # èƒ½åŠ›æ¡£æ¡ˆå­˜å‚¨
        self.capability_profiles: Dict[str, CapabilityProfile] = {}
        
        # æ€§èƒ½ç»Ÿè®¡
        self._performance_metrics = {
            "capability_detection_time": 0.0,
            "consensus_time": 0.0,
            "validation_time": 0.0,
        }
        
        logger.info("AI Commander å·²åˆå§‹åŒ–ï¼ˆè‡ªåŠ¨æ¨¡å¼ï¼‰")
        
        print(f"\nğŸ–ï¸ AI Commander å·²åˆå§‹åŒ–ï¼ˆè‡ªåŠ¨æ¨¡å¼ï¼‰")
    
    @property
    def capability_detector(self) -> CapabilityDetector:
        """å»¶è¿Ÿåˆå§‹åŒ–èƒ½åŠ›æ¢æµ‹å™¨"""
        if self._capability_detector is None:
            self._capability_detector = CapabilityDetector(
                self._commander_config.get("capability_detection", {})
            )
        return self._capability_detector
    
    @property
    def consensus_engine(self) -> ConsensusEngine:
        """å»¶è¿Ÿåˆå§‹åŒ–å…±è¯†å¼•æ“"""
        if self._consensus_engine is None:
            self._consensus_engine = ConsensusEngine(
                self._commander_config.get("consensus", {})
            )
        return self._consensus_engine
    
    @property
    def cross_validator(self) -> CrossValidator:
        """å»¶è¿Ÿåˆå§‹åŒ–äº¤å‰éªŒè¯å™¨"""
        if self._cross_validator is None:
            self._cross_validator = CrossValidator(
                self._commander_config.get("cross_validation", {})
            )
        return self._cross_validator
    
    def initialize_team(self, planner_result: Dict[str, Any]):
        """
        å¢å¼ºç‰ˆå›¢é˜Ÿåˆå§‹åŒ–
        
        åœ¨åŸºç¡€åˆå§‹åŒ–åï¼Œæ·»åŠ èƒ½åŠ›æ¢æµ‹å’Œæ™ºèƒ½åŒ¹é…
        """
        # è°ƒç”¨çˆ¶ç±»çš„åˆå§‹åŒ–
        super().initialize_team(planner_result)

        self._apply_mode()
        
        # èƒ½åŠ›æ¢æµ‹
        if self.enable_capability_detection:
            self._detect_agent_capabilities()

    def _apply_mode(self) -> None:
        complexity = self._estimate_task_complexity()
        if complexity == "simple":
            self.mode = "local"
            self.enable_consensus = False
            self.enable_cross_validation = False
        else:
            self.mode = "optimal"
            self.enable_consensus = True
            self.enable_cross_validation = True
        print(f"  æ¨¡å¼é€‰æ‹©: {self.mode} ({complexity})")
    
    def _detect_agent_capabilities(self):
        """æ¢æµ‹æ‰€æœ‰Agentçš„èƒ½åŠ›"""
        import time
        start_time = time.time()
        
        print(f"\nğŸ” æ­£åœ¨æ¢æµ‹å›¢é˜Ÿæˆå‘˜èƒ½åŠ›...")
        logger.info(f"å¼€å§‹èƒ½åŠ›æ¢æµ‹ï¼Œå›¢é˜Ÿè§„æ¨¡: {len(self.agents)}")
        
        # æ‰¹é‡æ¢æµ‹æ‰€æœ‰Agent
        agents_to_detect = list(self.agents)
        if getattr(self, "qa_agents", None):
            agents_to_detect.extend(self.qa_agents)
        
        for agent in agents_to_detect:
            if not hasattr(agent, 'role_name'):
                logger.warning(f"Agentç¼ºå°‘role_nameå±æ€§ï¼Œè·³è¿‡: {agent}")
                continue
                
            try:
                model_id = self._extract_model_id(agent)
                profile = self.capability_detector.quick_detect(
                    model_id=model_id,
                    model_config={}
                )
                self.capability_profiles[agent.role_name] = profile
                print(f"  âœ“ {agent.role_name}: {', '.join(profile.strengths)}")
                logger.info(f"æ¢æµ‹å®Œæˆ - {agent.role_name}: å¾—åˆ† {profile.scores}")
                
            except Exception as e:
                logger.error(f"æ¢æµ‹Agentèƒ½åŠ›æ—¶å‡ºé”™ ({agent.role_name}): {str(e)}")
                # ä½¿ç”¨é»˜è®¤æ¡£æ¡ˆ
                self.capability_profiles[agent.role_name] = self._get_default_profile(
                    agent.role_name
                )
        
        elapsed = time.time() - start_time
        self._performance_metrics["capability_detection_time"] = elapsed
        logger.info(f"èƒ½åŠ›æ¢æµ‹å®Œæˆï¼Œè€—æ—¶: {elapsed:.2f}ç§’")
    
    def _extract_model_id(self, agent: Agent) -> str:
        """æå–Agentçš„æ¨¡å‹ID"""
        if hasattr(agent, 'llm'):
            if hasattr(agent.llm, 'model_name'):
                return agent.llm.model_name
            elif hasattr(agent.llm, 'model'):
                return agent.llm.model
        return 'unknown'
    
    def _get_default_profile(self, agent_name: str) -> CapabilityProfile:
        """è·å–é»˜è®¤èƒ½åŠ›æ¡£æ¡ˆ"""
        return CapabilityProfile(
            model_id=f"{agent_name}_default",
            scores={"logic": 0.6, "creativity": 0.6, "code": 0.6, 
                   "analysis": 0.6, "communication": 0.6},
            strengths=[],
            weaknesses=[],
            optimal_temp=0.7,
            response_time=0.0,
            metadata={"type": "default"}
        )
    
    def run_collaboration(self, max_rounds: int = 5):
        """
        å¢å¼ºç‰ˆåä½œæµç¨‹
        
        åœ¨æ¯è½®è¿­ä»£åå¯é€‰å¯ç”¨å…±è¯†æœºåˆ¶å’Œäº¤å‰æ ¸æŸ¥
        """
        started_at = self._utcnow()
        self.run_reports = []
        start_round = self._resume_if_available(max_rounds)
        
        if not self.agents:
            print("âŒ é”™è¯¯: å›¢é˜Ÿæœªåˆå§‹åŒ–")
            report = self._build_report("no_engineers", started_at)
            self._write_report(report)
            return {
                "status": "error",
                "error": "no_engineers",
                "outputs": self.shared_memory.get_all_outputs(),
                "report": report,
            }

        review_report = self._ensure_review_artifacts()
        self.shared_memory.global_context["review_artifacts"] = review_report
        if review_report.get("status") not in ("passed", "skipped"):
            report = self._build_report("review_missing", started_at)
            report["review"] = review_report
            self._write_report(report)
            return {
                "status": "review_missing",
                "outputs": self.shared_memory.get_all_outputs(),
                "report": report,
            }
        
        if start_round > max_rounds:
            report = self._build_report("max_rounds_reached", started_at)
            self._write_report(report)
            self._save_resume_state(max_rounds, "max_rounds_reached")
            return {
                "status": "max_rounds_reached",
                "outputs": self.shared_memory.get_all_outputs(),
                "report": report,
            }

        ui_design_report = self._prepare_ui_design_assets()
        self.shared_memory.global_context["ui_design_report"] = ui_design_report
        allow_missing_ui = self._allow_missing_ui_baseline()
        if (
            ui_design_report.get("status") == "failed"
            and self.config.get("ui_design", {}).get("required", True)
            and not allow_missing_ui
        ):
            report = self._build_report("ui_design_failed", started_at)
            report["ui_design"] = ui_design_report
            self._write_report(report)
            self._save_resume_state(0, "ui_design_failed")
            return {
                "status": "ui_design_failed",
                "outputs": self.shared_memory.get_all_outputs(),
                "report": report,
            }

        print(f"\nğŸš€ AI Commander åä½œæµç¨‹å¯åŠ¨ (æœ€å¤§è½®æ¬¡: {max_rounds})...")
        
        run_status = "max_rounds_reached"
        testing_cfg = self.config.get("testing", {})
        testing_enabled = testing_cfg.get("enabled", True)
        
        for round_num in range(start_round, max_rounds + 1):
            print(f"\n{'='*60}")
            print(f"ğŸ”„ ç¬¬ {round_num} è½®è¿­ä»£")
            print(f"{'='*60}")
            
            round_report = {
                "round": round_num,
                "agents": [],
                "tests": {},
                "consensus": None,
                "validation": None,
                "qa_feedback_recorded": False,
            }
            self._round_saved_files = set()
            failure_reasons: List[str] = []
            
            # 1. Agentå·¥ä½œé˜¶æ®µ
            print("\nğŸ“ é˜¶æ®µ1: Agentå·¥ä½œ")
            max_workers = min(4, max(1, len(self.agents)))
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                futures = [executor.submit(self._run_agent, agent) for agent in self.agents]
                for future in as_completed(futures):
                    round_report["agents"].append(future.result())

            refactor_suggestions = self._collect_refactor_suggestions(self.output_dir)
            round_report["refactor_suggestions"] = refactor_suggestions
            self.shared_memory.global_context["refactor_suggestions"] = refactor_suggestions
            if refactor_suggestions:
                print("  âš ï¸ å‘ç°éœ€è¦æ‹†è§£çš„é•¿å‡½æ•°ï¼Œå·²ç”Ÿæˆå»ºè®®ã€‚")
            
            # 2. å…±è¯†æœºåˆ¶ï¼ˆå¯é€‰ï¼‰
            if self.enable_consensus and len(self.agents) > 1:
                print("\nğŸ¤ é˜¶æ®µ2: å…±è¯†è¾¾æˆ")
                consensus_result = self._reach_consensus(round_num)
                round_report["consensus"] = consensus_result.to_dict()
                if self._should_block_on_consensus(consensus_result):
                    failure_reasons.append("consensus_failed")
            
            # 3. æµ‹è¯•æ‰§è¡Œ
            print("\nğŸ§ª é˜¶æ®µ3: è‡ªåŠ¨åŒ–æµ‹è¯•")
            if testing_enabled:
                test_results = self.code_executor.run_tests(str(self.output_dir))
            else:
                test_results = "SKIPPED: Testing disabled by config."
            self.shared_memory.global_context["latest_test_results"] = test_results
            
            summary = test_results.splitlines()[0] if test_results else "No output"
            test_status = self._classify_test_result(test_results)
            self.shared_memory.global_context["latest_test_status"] = test_status
            
            round_report["tests"] = {"status": test_status, "summary": summary}
            print(f"  æµ‹è¯•çŠ¶æ€: {test_status}")

            ui_test_result = "SKIPPED: No UI tests."
            if self._requires_ui_baseline() and hasattr(self.code_executor, "run_ui_tests"):
                print("  ğŸ§ª [System] æ­£åœ¨æ‰§è¡Œ UI æµ‹è¯•...")
                ui_test_result = self.code_executor.run_ui_tests(str(self.output_dir))
            ui_test_status = self._classify_test_result(ui_test_result)
            round_report["ui_tests"] = {
                "status": ui_test_status,
                "summary": ui_test_result.splitlines()[0] if ui_test_result else "No output",
            }

            coverage_result = "SKIPPED: No coverage run."
            if hasattr(self.code_executor, "run_coverage"):
                print("  ğŸ§ª [System] æ­£åœ¨æ‰§è¡Œè¦†ç›–ç‡ç»Ÿè®¡...")
                coverage_result = self.code_executor.run_coverage(str(self.output_dir))
            coverage_status = self._classify_test_result(coverage_result)
            round_report["coverage"] = {
                "status": coverage_status,
                "summary": coverage_result.splitlines()[0] if coverage_result else "No output",
            }

            input_result = "SKIPPED: No input contract tests."
            if hasattr(self.code_executor, "run_input_contract_tests"):
                print("  ğŸ§ª [System] æ­£åœ¨æ‰§è¡Œè¾“å…¥å¥‘çº¦æµ‹è¯•...")
                input_result = self.code_executor.run_input_contract_tests(str(self.output_dir))
            input_status = self._classify_test_result(input_result)
            round_report["input_contract"] = {
                "status": input_status,
                "summary": input_result.splitlines()[0] if input_result else "No output",
            }
            
            # 4. äº¤å‰æ ¸æŸ¥ï¼ˆå¯é€‰ï¼‰
            if self.enable_cross_validation and len(self.agents) > 1:
                print("\nğŸ” é˜¶æ®µ4: äº¤å‰æ ¸æŸ¥")
                validation_report = self._cross_validate()
                round_report["validation"] = validation_report.to_dict()
                if self._should_block_on_validation(validation_report):
                    failure_reasons.append("validation_failed")
            
            # 5. QAå®¡æŸ¥
            if getattr(self, "qa_agents", None):
                print(f"\nğŸ‘¨â€ğŸ’¼ é˜¶æ®µ5: QAå®¡æŸ¥")
                qa_reports = []
                max_workers = min(4, max(1, len(self.qa_agents)))
                with ThreadPoolExecutor(max_workers=max_workers) as executor:
                    futures = [
                        executor.submit(
                            self._run_qa_agent,
                            agent,
                            round_num,
                            test_status,
                        )
                        for agent in self.qa_agents
                    ]
                for future in as_completed(futures):
                    qa_reports.append(future.result())
                round_report["qa_feedback"] = qa_reports
                round_report["qa_feedback_recorded"] = True
                qa_gate_cfg = self.config.get("quality_gates", {}).get("qa", {})
                if qa_gate_cfg.get("enabled", True) and self._qa_feedback_failed(qa_reports):
                    failure_reasons.append("qa_failed")
            
            # 6. åˆ¤æ–­æ˜¯å¦ç»“æŸ
            if test_status == "passed":
                print("\nâœ¨ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
                if input_status in ("failed", "error"):
                    print("  âŒ è¾“å…¥å¥‘çº¦æµ‹è¯•æœªé€šè¿‡ï¼Œè¿›å…¥ä¿®å¤å›åˆã€‚")
                    failure_reasons.append("input_contract_failed")
                if self._requires_ui_baseline() and self._should_require_ui_tests():
                    if ui_test_status in ("failed", "error", "skipped", "unknown"):
                        print("  âŒ UI æµ‹è¯•æœªé€šè¿‡æˆ–ç¼ºå¤±ï¼Œè¿›å…¥ä¿®å¤å›åˆã€‚")
                        failure_reasons.append("ui_tests_failed")
                if self._should_require_coverage():
                    if coverage_status in ("failed", "error", "skipped", "unknown"):
                        print("  âŒ è¦†ç›–ç‡ç»Ÿè®¡æœªé€šè¿‡æˆ–ç¼ºå¤±ï¼Œè¿›å…¥ä¿®å¤å›åˆã€‚")
                        failure_reasons.append("coverage_failed")
                if self._requires_ui_baseline():
                    ui_check = self._check_ui_evidence()
                    round_report["ui_evidence"] = ui_check
                    if ui_check["status"] != "passed":
                        print("  âŒ UI è¯æ®ä¸å®Œæ•´ï¼Œè¿›å…¥ä¿®å¤å›åˆã€‚")
                        failure_reasons.append("ui_evidence_missing")
                sim_result = "SKIPPED: No user simulation."
                if hasattr(self.code_executor, "run_user_simulation"):
                    print("  ğŸ§­ [System] æ­£åœ¨æ‰§è¡Œç”¨æˆ·æ¨¡æ‹Ÿæµ‹è¯•...")
                    sim_result = self.code_executor.run_user_simulation(str(self.output_dir))
                sim_status = self._classify_test_result(sim_result)
                round_report["user_simulation"] = {
                    "status": sim_status,
                    "summary": sim_result.splitlines()[0] if sim_result else "No output",
                }
                if self._requires_ui_baseline() and self._should_require_ui_simulation():
                    if sim_status not in ("passed",):
                        print("  âŒ ç”¨æˆ·æ¨¡æ‹Ÿæµ‹è¯•ç¼ºå¤±æˆ–å¤±è´¥ï¼Œè¿›å…¥ä¿®å¤å›åˆã€‚")
                        failure_reasons.append("user_simulation_failed")
                elif sim_status not in ("passed", "skipped", "unknown"):
                    print("  âŒ ç”¨æˆ·æ¨¡æ‹Ÿæµ‹è¯•æœªé€šè¿‡ï¼Œè¿›å…¥ä¿®å¤å›åˆã€‚")
                    failure_reasons.append("user_simulation_failed")
                acceptance_criteria = self._get_acceptance_criteria(self._get_requirements_payload())
                acceptance_report = self._verify_acceptance_checklist(acceptance_criteria)
                round_report["acceptance"] = acceptance_report
                if acceptance_report.get("status") == "failed":
                    print("  âŒ éªŒæ”¶æ¸…å•æœªå®Œæˆï¼Œè¿›å…¥ä¿®å¤å›åˆã€‚")
                    failure_reasons.append("acceptance_failed")
                if not failure_reasons:
                    self._write_evidence_manifest(round_report)
                    self._archive_evidence()
                    approved = self._run_approval_gate(round_num, test_status, round_report)
                    if approved:
                        run_status = "passed"
                        self.run_reports.append(round_report)
                        break
                    print("  âš ï¸ å®¡æ‰¹æœªé€šè¿‡ï¼Œè¿›å…¥ä¿®å¤å›åˆã€‚")
                    failure_reasons.append("approval_failed")

            else:
                if test_status in ("failed", "error"):
                    failure_reasons.append("tests_failed")
                elif test_status in ("skipped", "unknown") and self._should_require_tests():
                    failure_reasons.append("tests_failed")
                if self._requires_ui_baseline() and self._should_require_ui_tests():
                    if ui_test_status in ("failed", "error", "skipped", "unknown"):
                        failure_reasons.append("ui_tests_failed")
                if self._should_require_coverage():
                    if coverage_status in ("failed", "error", "skipped", "unknown"):
                        failure_reasons.append("coverage_failed")

            if failure_reasons:
                round_report["failure_reasons"] = failure_reasons
                self._write_bug_card(failure_reasons[0], round_report)
                self._write_evidence_manifest(round_report)
                self._archive_evidence()
                self.run_reports.append(round_report)
                self._save_resume_state(round_num, "in_progress")
                self._sync_iteration_artifacts()
                run_status = failure_reasons[0]
                if round_num < max_rounds:
                    continue
                break
            
            self.run_reports.append(round_report)
            self._save_resume_state(round_num, "in_progress")
            self._sync_iteration_artifacts()
        
        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        report = self._build_report(run_status, started_at)
        self._write_report(report)
        self._save_resume_state(len(self.run_reports), run_status)
        self._sync_iteration_artifacts()
        
        return {
            "status": run_status,
            "outputs": self.shared_memory.get_all_outputs(),
            "report": report,
            "capability_profiles": {
                name: profile.to_dict() 
                for name, profile in self.capability_profiles.items()
            }
        }
    
    def _run_agent(self, agent: Agent) -> Dict[str, Any]:
        """è¿è¡Œå•ä¸ªAgent"""
        return self._run_agent_once(agent)
    
    def _reach_consensus(self, round_num: int) -> ConsensusResult:
        """è¾¾æˆå…±è¯†"""
        import time
        start_time = time.time()
        
        try:
            # è·å–æ‰€æœ‰Agentçš„è¾“å‡º
            all_outputs = self.shared_memory.get_all_outputs()
            
            # ç¡®å®šå…±è¯†ç­–ç•¥
            task_complexity = self._estimate_task_complexity()
            strategy = self.consensus_engine.auto_select_strategy(
                self.agents,
                task_complexity
            )
            
            logger.info(f"ç¬¬{round_num}è½®å…±è¯†ï¼Œç­–ç•¥: {strategy}, å¤æ‚åº¦: {task_complexity}")
            
            # æ„å»ºæƒé‡ï¼ˆåŸºäºèƒ½åŠ›æ¡£æ¡ˆï¼‰
            weights = self._calculate_agent_weights()
            
            # è¾¾æˆå…±è¯†
            consensus_result = self.consensus_engine.reach_consensus(
                agents=self.agents,
                task=f"Round {round_num} collaboration",
                strategy=strategy,
                weights=weights
            )
            
            elapsed = time.time() - start_time
            self._performance_metrics["consensus_time"] += elapsed
            logger.info(f"å…±è¯†è¾¾æˆå®Œæˆï¼Œè€—æ—¶: {elapsed:.2f}ç§’ï¼Œç½®ä¿¡åº¦: {consensus_result.confidence}")
            
            return consensus_result
            
        except Exception as e:
            logger.error(f"å…±è¯†è¾¾æˆå¤±è´¥: {str(e)}")
            # è¿”å›é»˜è®¤å…±è¯†ç»“æœ
            return ConsensusResult(
                final_decision="ç»§ç»­åä½œ",
                confidence=0.5,
                votes={},
                strategy="fallback",
                rounds=1
            )
    
    def _calculate_agent_weights(self) -> Dict[str, float]:
        """è®¡ç®—Agentæƒé‡ï¼ˆåŸºäºèƒ½åŠ›æ¡£æ¡ˆï¼‰"""
        weights = {}
        for agent in self.agents:
            agent_name = agent.role_name if hasattr(agent, 'role_name') else str(agent)
            if agent_name in self.capability_profiles:
                profile = self.capability_profiles[agent_name]
                # åŸºäºä»£ç å’Œé€»è¾‘èƒ½åŠ›çš„åŠ æƒå¹³å‡
                weights[agent_name] = (
                    profile.get_score("code") * 0.6 + 
                    profile.get_score("logic") * 0.4
                )
            else:
                weights[agent_name] = 1.0
        
        # å½’ä¸€åŒ–æƒé‡
        total_weight = sum(weights.values())
        if total_weight > 0:
            weights = {k: v/total_weight for k, v in weights.items()}
        
        return weights
    
    def _cross_validate(self) -> ValidationReport:
        """æ‰§è¡Œäº¤å‰æ ¸æŸ¥"""
        import time
        start_time = time.time()
        
        try:
            # è·å–æ‰€æœ‰Agentçš„è¾“å‡º
            all_outputs = self.shared_memory.get_all_outputs()
            
            # æå–æ¯ä¸ªAgentçš„æœ€æ–°è¾“å‡º
            results = self._extract_latest_outputs(all_outputs)
            
            if len(results) < 2:
                logger.warning("è¾“å‡ºæ•°é‡ä¸è¶³ï¼Œè·³è¿‡äº¤å‰æ ¸æŸ¥")
                return self._create_empty_validation_report()
            
            logger.info(f"å¼€å§‹äº¤å‰æ ¸æŸ¥ï¼Œç»“æœæ•°: {len(results)}")
            
            # æ‰§è¡Œäº¤å‰æ ¸æŸ¥
            validation_report = self.cross_validator.validate(
                results=results,
                agents=self.agents,
                anonymous=True
            )
            
            elapsed = time.time() - start_time
            self._performance_metrics["validation_time"] += elapsed
            logger.info(f"äº¤å‰æ ¸æŸ¥å®Œæˆï¼Œè€—æ—¶: {elapsed:.2f}ç§’ï¼Œå†²çªæ•°: {len(validation_report.conflicts)}")
            
            return validation_report
            
        except Exception as e:
            logger.error(f"äº¤å‰æ ¸æŸ¥å¤±è´¥: {str(e)}")
            return self._create_empty_validation_report()
    
    def _extract_latest_outputs(self, all_outputs: Dict[str, List[Any]]) -> Dict[str, Any]:
        """æå–æ‰€æœ‰Agentçš„æœ€æ–°è¾“å‡º"""
        results = {}
        for agent in self.agents:
            agent_name = agent.role_name if hasattr(agent, 'role_name') else str(agent)
            if agent_name in all_outputs and all_outputs[agent_name]:
                results[agent_name] = all_outputs[agent_name][-1]
        return results
    
    def _create_empty_validation_report(self) -> ValidationReport:
        """åˆ›å»ºç©ºçš„éªŒè¯æŠ¥å‘Š"""
        from agents.dev_team.commander.cross_validator import ValidationReport
        return ValidationReport(
            reviews={},
            conflicts=[],
            consistency_score=1.0,
            is_valid=True
        )
    
    def _estimate_task_complexity(self) -> str:
        """
        ä¼°ç®—ä»»åŠ¡å¤æ‚åº¦
        
        åŸºäºéœ€æ±‚é•¿åº¦å’ŒAgentæ•°é‡çš„å¯å‘å¼ä¼°ç®—
        """
        requirements = self.shared_memory.global_context.get("requirements", "")
        
        if isinstance(requirements, str):
            req_length = len(requirements)
        else:
            req_length = len(str(requirements))
        
        agent_count = len(self.agents)
        
        if req_length > 1000 or agent_count >= 5:
            return "complex"
        elif req_length > 500 or agent_count >= 3:
            return "medium"
        else:
            return "simple"
    
    def get_capability_summary(self) -> Dict[str, Any]:
        """è·å–å›¢é˜Ÿèƒ½åŠ›æ€»ç»“"""
        if not self.capability_profiles:
            return {"message": "èƒ½åŠ›æ¢æµ‹æœªå¯ç”¨"}
        
        # è®¡ç®—å›¢é˜Ÿç»Ÿè®¡ä¿¡æ¯
        all_scores = {category: [] for category in ["logic", "creativity", "code", "analysis", "communication"]}
        
        for profile in self.capability_profiles.values():
            for category, score in profile.scores.items():
                if category in all_scores:
                    all_scores[category].append(score)
        
        team_avg_scores = {
            category: sum(scores) / len(scores) if scores else 0.0
            for category, scores in all_scores.items()
        }
        
        summary = {
            "team_size": len(self.capability_profiles),
            "team_average_scores": team_avg_scores,
            "team_strengths": [cat for cat, score in team_avg_scores.items() if score >= 0.7],
            "team_weaknesses": [cat for cat, score in team_avg_scores.items() if score < 0.5],
            "profiles": {},
            "performance_metrics": self._performance_metrics
        }
        
        for name, profile in self.capability_profiles.items():
            summary["profiles"][name] = {
                "model": profile.model_id,
                "strengths": profile.strengths,
                "weaknesses": profile.weaknesses,
                "scores": profile.scores,
                "optimal_temp": profile.optimal_temp
            }
        
        return summary
    
    def save_capability_profiles(self, filepath: Optional[Path] = None):
        """ä¿å­˜èƒ½åŠ›æ¡£æ¡ˆåˆ°æ–‡ä»¶"""
        if not filepath:
            filepath = self.output_dir / "capability_profiles.json"
        
        profiles_data = {
            name: profile.to_dict()
            for name, profile in self.capability_profiles.items()
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(profiles_data, f, ensure_ascii=False, indent=2)
        
        print(f"  ğŸ’¾ èƒ½åŠ›æ¡£æ¡ˆå·²ä¿å­˜: {filepath}")
