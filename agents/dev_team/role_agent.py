import json
from pathlib import Path
from typing import Any, Dict
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate
from agents.dev_team.interfaces import MemoryStore


class RoleAgent:
    def __init__(self, role_jd: Dict[str, Any], config: Dict[str, Any], memory: MemoryStore, output_dir: Path = None):
        self.role_name = role_jd.get("role_name", "Unknown")
        self.role_type = role_jd.get("role_type", "ENGINEER").upper() # Default to ENGINEER
        self.role_jd = role_jd
        self.config = config
        self.memory = memory
        self.output_dir = output_dir

        # åˆå§‹åŒ– LLM - æ ¹æ® role_type å†³å®š temperature
        llm_cfg = config.get("llm", {})
        
        # å…¼å®¹æ—§é€»è¾‘ï¼šå¦‚æœ role_type æ˜¯ ENGINEER ä½†åå­—é‡Œæœ‰ QA/Testï¼Œä¹Ÿå½“ä½œ QA
        role_upper = self.role_name.upper()
        self.is_qa = self.role_type == "QA" or "QA" in role_upper or "TEST" in role_upper
        
        temp = 0.3 if self.is_qa else 0.7

        self.llm = ChatOpenAI(
            model=llm_cfg.get("model", "gpt-4o"),
            api_key=llm_cfg.get("api_key"),
            base_url=llm_cfg.get("api_base"),
            temperature=temp
        )
        
        # åŠ è½½ System Prompt æ¨¡æ¿
        # ä¼˜å…ˆä½¿ç”¨ role_type å¯¹åº”çš„ prompt
        if self.is_qa:
            prompt_path_str = self.config["roles"]["qa"]["prompt_path"]
        else:
            prompt_path_str = self.config["roles"]["engineer"]["prompt_path"]
            
        # åŠ¨æ€å¤„ç†è·¯å¾„
        # Assuming config["agent_root"] might be available or we resolve relative
        base_dir = Path(config.get("agent_root", "."))
        if not Path(prompt_path_str).is_absolute():
             prompt_path = base_dir / prompt_path_str
        else:
             prompt_path = Path(prompt_path_str)

        with open(prompt_path, "r", encoding="utf-8") as f:
            self.system_prompt_template = f.read()
        self.prompt_template = ChatPromptTemplate.from_template(self.system_prompt_template)

    # extract_and_save_files method removed (Decoupled IO)

    def run(self) -> str:
        # ä»å…±äº«è®°å¿†è·å–ä¸Šä¸‹æ–‡
        context = self.memory.get_context_for_role(self.role_name, self.role_type)

        # æ¸²æŸ“ Prompt
        # æ ¹æ®ä¸åŒè§’è‰²çš„ Prompt éœ€è¦çš„å˜é‡è¿›è¡Œå¡«å……
        # Engineer prompt éœ€è¦: role_jd, requirements
        # QA prompt éœ€è¦: role_jd, engineer_output
        # QA prompt éœ€è¦: role_jd, engineer_output
        
        engineer_output = {}
        if self.is_qa:
            engineer_output = self.memory.get_peer_output_summaries(self.role_name, include_qa=False)

        prompt_kwargs = {
            "role_jd": json.dumps(self.role_jd, ensure_ascii=False, indent=2),
            "requirements": self.memory.global_context.get("requirements", ""),
            "engineer_output": json.dumps(engineer_output, ensure_ascii=False),
            "test_results": self.memory.global_context.get("latest_test_results", "æš‚æ— æµ‹è¯•è¿è¡Œç»“æœ"),
            "bug_cards": json.dumps(self.memory.global_context.get("bug_cards", []), ensure_ascii=False),
        }
        
        system_instruction = self.prompt_template.format(**prompt_kwargs)

        user_content = (
            f"ã€å½“å‰ä»»åŠ¡ã€‘\nä½ ç°åœ¨çš„è§’è‰²æ˜¯: {self.role_name}\n"
            f"ä½ çš„èŒè´£æ˜¯: {json.dumps(self.role_jd.get('responsibilities', []), ensure_ascii=False)}\n\n"
            f"ã€å…±äº«ä¸Šä¸‹æ–‡ã€‘\n{context}\n\n"
            f"è¯·å¼€å§‹ä½ çš„å·¥ä½œã€‚å¦‚æœéœ€è¦ç¼–å†™ä»£ç ï¼Œè¯·åŠ¡å¿…ä½¿ç”¨ <file path='...'>...</file> æ ¼å¼åŒ…è£¹ä»£ç ã€‚"
        )

        messages = [
            SystemMessage(content=system_instruction),
            HumanMessage(content=user_content)
        ]

        print(f"\nğŸ¤– [{self.role_name}] æ­£åœ¨æ€è€ƒå¹¶è¾“å‡º...\n")
        
        full_response = ""
        try:
            # ä½¿ç”¨æµå¼è¾“å‡º
            for chunk in self.llm.stream(messages):
                content = chunk.content
                if isinstance(content, str):
                    print(content, end="", flush=True)
                    full_response += content
                elif isinstance(content, list):
                    for item in content:
                        if isinstance(item, str):
                            print(item, end="", flush=True)
                            full_response += item
                        elif isinstance(item, dict) and "text" in item:
                            text = item["text"]
                            print(text, end="", flush=True)
                            full_response += text
            
            print("\n") # æ¢è¡Œ

        except Exception as e:
            error_msg = f"Agent execution failed: {str(e)}"
            print(f"\nâŒ {error_msg}")
            full_response = error_msg

        # å°è¯•æå–å¹¶ä¿å­˜æ–‡ä»¶ -> moved to coordinator
        # self.extract_and_save_files(full_response)

        # å°†è¾“å‡ºå­˜å…¥å…±äº«è®°å¿†
        self.memory.add_output(self.role_name, full_response)
        return str(full_response)
